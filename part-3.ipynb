{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question-3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the performance of three types of recurrent neural network architectures for classifying human activity from movement signals measured with three sensors simultaneously: Recurrent Neural Network (RNN), Long Short-Term Memory (LSTM), and Gated Recurrent Unit (GRU). Back propagation through time is used to train the network and a multi-layer perceptron with a softmax function is used for classification. I ran the network with two hidden dimension settings, 128 and 32 neurons in the hidden layer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Validation Loss: 5.7308, Validation Accuracy: 0.2400\n",
      "Epoch: 1, Validation Loss: 6.4871, Validation Accuracy: 0.3767\n",
      "Epoch: 2, Validation Loss: 10.7426, Validation Accuracy: 0.3633\n",
      "Epoch: 3, Validation Loss: 7.3220, Validation Accuracy: 0.2200\n",
      "Epoch: 4, Validation Loss: 10.6470, Validation Accuracy: 0.2967\n",
      "Epoch: 5, Validation Loss: 10.3178, Validation Accuracy: 0.2767\n",
      "Early stopping at epoch: 5\n",
      "Train Loss: 10.3030\n",
      "Confusion Matrix - Train Set:\n",
      " [[147   2  44  69 119  71]\n",
      " [ 74  13  66  23 208  67]\n",
      " [ 42   0 238   7 155   3]\n",
      " [188   0  44 132  74   4]\n",
      " [119   1  44  62 153  77]\n",
      " [115   3  48  61 135  92]]\n"
     ]
    }
   ],
   "source": [
    "# 3-a\n",
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        # Xavier initialization\n",
    "        self.wh = np.random.randn(hidden_dim, hidden_dim) / np.sqrt(hidden_dim)\n",
    "        self.wx = np.random.randn(hidden_dim, input_dim) / np.sqrt(input_dim)\n",
    "        self.wy = np.random.randn(output_dim, hidden_dim) / np.sqrt(hidden_dim)\n",
    "        self.bh = np.zeros((hidden_dim, 1))\n",
    "        self.by = np.zeros((output_dim, 1))\n",
    "        # Momentum initializations\n",
    "        self.m_wx, self.m_wh, self.m_wy = np.zeros_like(self.wx), np.zeros_like(self.wh), np.zeros_like(self.wy)\n",
    "        self.m_bh, self.m_by = np.zeros_like(self.bh), np.zeros_like(self.by)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        h = np.zeros((self.wh.shape[0], 1)) \n",
    "        self.last_inputs = inputs\n",
    "        self.last_hs = { 0: h }\n",
    "\n",
    "        # Perform forward pass through time step\n",
    "        for i, x in enumerate(inputs):\n",
    "            h = np.tanh(self.wx @ x.reshape(-1, 1) + self.wh @ h + self.bh)\n",
    "            self.last_hs[i + 1] = h\n",
    "        y = self.wy @ h + self.by\n",
    "\n",
    "        return y, h\n",
    "\n",
    "    def backward(self, d_y, learn_rate=2e-2, momentum=0.85):\n",
    "        n = self.wy.shape[0]\n",
    "\n",
    "        # Calculate gradient of output wrt Wy and by\n",
    "        d_wy = d_y @ self.last_hs[len(self.last_inputs)].T\n",
    "        d_by = d_y\n",
    "\n",
    "        # Initialize dh_next and gradients for Wx, Wh, bh\n",
    "        dh_next = self.wy.T @ d_y\n",
    "        d_wx = np.zeros_like(self.wx)\n",
    "        d_wh = np.zeros_like(self.wh)\n",
    "        d_bh = np.zeros_like(self.bh)\n",
    "\n",
    "        # Backpropagation through time\n",
    "        for t in reversed(range(len(self.last_inputs))):\n",
    "            temp = ((1 - self.last_hs[t + 1] ** 2) * dh_next)\n",
    "\n",
    "            # Accumulate gradients for Wx, Wh, and bh\n",
    "            d_wx += temp @ self.last_inputs[t].reshape(1,-1)\n",
    "            d_wh += temp @ self.last_hs[t].T\n",
    "            d_bh += temp\n",
    "\n",
    "            # Next dh_next\n",
    "            dh_next = self.wh @ temp\n",
    "\n",
    "        # Update weights and biases using SGD update with momentum\n",
    "        self.m_wx = momentum * self.m_wx + learn_rate * d_wx\n",
    "        self.wx -= self.m_wx\n",
    "        self.m_wh = momentum * self.m_wh + learn_rate * d_wh\n",
    "        self.wh -= self.m_wh\n",
    "        self.m_wy = momentum * self.m_wy + learn_rate * d_wy\n",
    "        self.wy -= self.m_wy\n",
    "        self.m_bh = momentum * self.m_bh + learn_rate * d_bh\n",
    "        self.bh -= self.m_bh\n",
    "        self.m_by = momentum * self.m_by + learn_rate * d_by\n",
    "        self.by -= self.m_by\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "# Loss function\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    return -np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "# Load dataset\n",
    "def load_data(filename):\n",
    "    with h5py.File(filename, 'r') as hf:\n",
    "        trainX = np.array(hf.get('trX'))\n",
    "        trainY = np.array(hf.get('trY'))\n",
    "        testX = np.array(hf.get('tstX'))\n",
    "        testY = np.array(hf.get('tstY'))\n",
    "    return trainX, trainY, testX, testY\n",
    "\n",
    "def convert_to_one_hot(y, C):\n",
    "    return np.eye(C)[y.reshape(-1)].T\n",
    "\n",
    "def process_data(trainX, trainY, testX, testY):\n",
    "    assert trainX.shape[0] == trainY.shape[0], \"Mismatch in number of training samples\"\n",
    "    assert testX.shape[0] == testY.shape[0], \"Mismatch in number of test samples\"\n",
    "\n",
    "    # Normalize the data\n",
    "    mean = np.mean(trainX, axis=0)\n",
    "    std = np.std(trainX, axis=0)\n",
    "\n",
    "    trainX = (trainX - mean) / std\n",
    "    testX = (testX - mean) / std\n",
    "\n",
    "    # Convert labels to integers\n",
    "    trainY = trainY.astype(int)\n",
    "    testY = testY.astype(int)\n",
    "\n",
    "    # One-hot encoding\n",
    "    #trainY = convert_to_one_hot(trainY, 6).T\n",
    "    #testY = convert_to_one_hot(testY, 6).T\n",
    "\n",
    "    return trainX, trainY, testX, testY\n",
    "\n",
    "\n",
    "\n",
    "def run_model(trainX, trainY, testX, testY, hidden_dim=128, epochs=50, mini_batch_size=32, early_stop_epochs=5):\n",
    "    rnn = RNN(trainX.shape[2], hidden_dim, trainY.shape[1])\n",
    "\n",
    "    # Split training data into training and validation set\n",
    "    trainX, valX, trainY, valY = train_test_split(trainX, trainY, test_size=0.1)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    stop_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Mini-batch gradient descent\n",
    "        mini_batches = [(trainX[k:k+mini_batch_size], trainY[k:k+mini_batch_size]) \n",
    "                        for k in range(0, trainX.shape[0], mini_batch_size)]\n",
    "        \n",
    "        for mini_batch in mini_batches:\n",
    "            X_mini, Y_mini = mini_batch\n",
    "            for x, y_true in zip(X_mini, Y_mini):\n",
    "                y, _ = rnn.forward(x)\n",
    "                y = softmax(y)\n",
    "                error = y - y_true.reshape(-1,1)\n",
    "                rnn.backward(error)\n",
    "\n",
    "        # Compute validation loss and accuracy\n",
    "        val_loss = 0\n",
    "        pred_val = []\n",
    "        true_val = []\n",
    "        for x, y_true in zip(valX, valY):\n",
    "            y, _ = rnn.forward(x)\n",
    "            y = softmax(y)\n",
    "            val_loss += cross_entropy_loss(y, y_true.reshape(-1,1))\n",
    "            pred_val.append(np.argmax(y))\n",
    "            true_val.append(np.argmax(y_true))\n",
    "\n",
    "        # Calculate overall accuracy for each epoch\n",
    "        val_accuracy = accuracy_score(true_val, pred_val)\n",
    "        print('Epoch: %d, Validation Loss: %.4f, Validation Accuracy: %.4f' % \n",
    "              (epoch, val_loss / valX.shape[0], val_accuracy))\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            stop_counter = 0\n",
    "        else:\n",
    "            stop_counter += 1\n",
    "\n",
    "        if stop_counter == early_stop_epochs:\n",
    "            print('Early stopping at epoch: %d' % epoch)\n",
    "            break\n",
    "\n",
    "    # Train set\n",
    "    train_loss = 0\n",
    "    pred_train = []\n",
    "    true_train = []\n",
    "    for x, y_true in zip(trainX, trainY):\n",
    "        y, _ = rnn.forward(x)\n",
    "        y = softmax(y)\n",
    "        train_loss += cross_entropy_loss(y, y_true.reshape(-1,1))\n",
    "        pred_train.append(np.argmax(y))\n",
    "        true_train.append(np.argmax(y_true))\n",
    "    print('Train Loss: %.4f' % (train_loss / trainX.shape[0]))\n",
    "    print(\"Confusion Matrix - Train Set:\\n\", confusion_matrix(true_train, pred_train))\n",
    "\n",
    "\n",
    "trainX, trainY, testX, testY = load_data('data3.h5')\n",
    "trainX, trainY, testX, testY = process_data(trainX, trainY, testX, testY)\n",
    "run_model(trainX, trainY, testX, testY)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) (for hidden_dim = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Validation Loss: 2.4639, Validation Accuracy: 0.2933\n",
      "Epoch: 1, Validation Loss: 2.4120, Validation Accuracy: 0.3933\n",
      "Epoch: 2, Validation Loss: 2.2921, Validation Accuracy: 0.3067\n",
      "Epoch: 3, Validation Loss: 2.2706, Validation Accuracy: 0.3700\n",
      "Epoch: 4, Validation Loss: 3.4950, Validation Accuracy: 0.2167\n",
      "Epoch: 5, Validation Loss: 2.6243, Validation Accuracy: 0.3000\n",
      "Epoch: 6, Validation Loss: 2.7891, Validation Accuracy: 0.3300\n",
      "Epoch: 7, Validation Loss: 2.0395, Validation Accuracy: 0.3333\n",
      "Epoch: 8, Validation Loss: 2.9833, Validation Accuracy: 0.2533\n",
      "Epoch: 9, Validation Loss: 2.1638, Validation Accuracy: 0.2867\n",
      "Epoch: 10, Validation Loss: 1.8491, Validation Accuracy: 0.2967\n",
      "Epoch: 11, Validation Loss: 1.9860, Validation Accuracy: 0.3767\n",
      "Epoch: 12, Validation Loss: 2.7899, Validation Accuracy: 0.2733\n",
      "Epoch: 13, Validation Loss: 4.3805, Validation Accuracy: 0.1967\n",
      "Epoch: 14, Validation Loss: 2.9072, Validation Accuracy: 0.2867\n",
      "Epoch: 15, Validation Loss: 3.0753, Validation Accuracy: 0.2800\n",
      "Early stopping at epoch: 15\n",
      "Train Loss: 2.7982\n",
      "Confusion Matrix - Train Set:\n",
      " [[  1  28  23   0 391   0]\n",
      " [ 10  94  40   0 302   0]\n",
      " [  0  54 345   0  57   0]\n",
      " [  0   1   5   0 445   0]\n",
      " [  3  28  23   0 398   0]\n",
      " [  1  30  15   0 404   2]]\n"
     ]
    }
   ],
   "source": [
    "# 3-a\n",
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        # Xavier initialization\n",
    "        self.wh = np.random.randn(hidden_dim, hidden_dim) / np.sqrt(hidden_dim)\n",
    "        self.wx = np.random.randn(hidden_dim, input_dim) / np.sqrt(input_dim)\n",
    "        self.wy = np.random.randn(output_dim, hidden_dim) / np.sqrt(hidden_dim)\n",
    "        self.bh = np.zeros((hidden_dim, 1))\n",
    "        self.by = np.zeros((output_dim, 1))\n",
    "        # Momentum initializations\n",
    "        self.m_wx, self.m_wh, self.m_wy = np.zeros_like(self.wx), np.zeros_like(self.wh), np.zeros_like(self.wy)\n",
    "        self.m_bh, self.m_by = np.zeros_like(self.bh), np.zeros_like(self.by)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        h = np.zeros((self.wh.shape[0], 1)) \n",
    "        self.last_inputs = inputs\n",
    "        self.last_hs = { 0: h }\n",
    "\n",
    "        # Perform forward pass through time step\n",
    "        for i, x in enumerate(inputs):\n",
    "            h = np.tanh(self.wx @ x.reshape(-1, 1) + self.wh @ h + self.bh)\n",
    "            self.last_hs[i + 1] = h\n",
    "        y = self.wy @ h + self.by\n",
    "\n",
    "        return y, h\n",
    "\n",
    "    def backward(self, d_y, learn_rate=2e-2, momentum=0.85):\n",
    "        n = self.wy.shape[0]\n",
    "\n",
    "        # Calculate gradient of output wrt Wy and by\n",
    "        d_wy = d_y @ self.last_hs[len(self.last_inputs)].T\n",
    "        d_by = d_y\n",
    "\n",
    "        # Initialize dh_next and gradients for Wx, Wh, bh\n",
    "        dh_next = self.wy.T @ d_y\n",
    "        d_wx = np.zeros_like(self.wx)\n",
    "        d_wh = np.zeros_like(self.wh)\n",
    "        d_bh = np.zeros_like(self.bh)\n",
    "\n",
    "        # Backpropagation through time\n",
    "        for t in reversed(range(len(self.last_inputs))):\n",
    "            temp = ((1 - self.last_hs[t + 1] ** 2) * dh_next)\n",
    "\n",
    "            # Accumulate gradients for Wx, Wh, and bh\n",
    "            d_wx += temp @ self.last_inputs[t].reshape(1,-1)\n",
    "            d_wh += temp @ self.last_hs[t].T\n",
    "            d_bh += temp\n",
    "\n",
    "            # Next dh_next\n",
    "            dh_next = self.wh @ temp\n",
    "\n",
    "        # Update weights and biases using SGD update with momentum\n",
    "        self.m_wx = momentum * self.m_wx + learn_rate * d_wx\n",
    "        self.wx -= self.m_wx\n",
    "        self.m_wh = momentum * self.m_wh + learn_rate * d_wh\n",
    "        self.wh -= self.m_wh\n",
    "        self.m_wy = momentum * self.m_wy + learn_rate * d_wy\n",
    "        self.wy -= self.m_wy\n",
    "        self.m_bh = momentum * self.m_bh + learn_rate * d_bh\n",
    "        self.bh -= self.m_bh\n",
    "        self.m_by = momentum * self.m_by + learn_rate * d_by\n",
    "        self.by -= self.m_by\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "# Loss function\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    return -np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "# Load dataset\n",
    "def load_data(filename):\n",
    "    with h5py.File(filename, 'r') as hf:\n",
    "        trainX = np.array(hf.get('trX'))\n",
    "        trainY = np.array(hf.get('trY'))\n",
    "        testX = np.array(hf.get('tstX'))\n",
    "        testY = np.array(hf.get('tstY'))\n",
    "    return trainX, trainY, testX, testY\n",
    "\n",
    "def convert_to_one_hot(y, C):\n",
    "    return np.eye(C)[y.reshape(-1)].T\n",
    "\n",
    "def process_data(trainX, trainY, testX, testY):\n",
    "    assert trainX.shape[0] == trainY.shape[0], \"Mismatch in number of training samples\"\n",
    "    assert testX.shape[0] == testY.shape[0], \"Mismatch in number of test samples\"\n",
    "\n",
    "    # Normalize the data\n",
    "    mean = np.mean(trainX, axis=0)\n",
    "    std = np.std(trainX, axis=0)\n",
    "\n",
    "    trainX = (trainX - mean) / std\n",
    "    testX = (testX - mean) / std\n",
    "\n",
    "    # Convert labels to integers\n",
    "    trainY = trainY.astype(int)\n",
    "    testY = testY.astype(int)\n",
    "\n",
    "    # One-hot encoding\n",
    "    #trainY = convert_to_one_hot(trainY, 6).T\n",
    "    #testY = convert_to_one_hot(testY, 6).T\n",
    "\n",
    "    return trainX, trainY, testX, testY\n",
    "\n",
    "\n",
    "\n",
    "def run_model(trainX, trainY, testX, testY, hidden_dim=32, epochs=50, mini_batch_size=32, early_stop_epochs=5):\n",
    "    rnn = RNN(trainX.shape[2], hidden_dim, trainY.shape[1])\n",
    "\n",
    "    # Split training data into training and validation set\n",
    "    trainX, valX, trainY, valY = train_test_split(trainX, trainY, test_size=0.1)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    stop_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Mini-batch gradient descent\n",
    "        mini_batches = [(trainX[k:k+mini_batch_size], trainY[k:k+mini_batch_size]) \n",
    "                        for k in range(0, trainX.shape[0], mini_batch_size)]\n",
    "        \n",
    "        for mini_batch in mini_batches:\n",
    "            X_mini, Y_mini = mini_batch\n",
    "            for x, y_true in zip(X_mini, Y_mini):\n",
    "                y, _ = rnn.forward(x)\n",
    "                y = softmax(y)\n",
    "                error = y - y_true.reshape(-1,1)\n",
    "                rnn.backward(error)\n",
    "\n",
    "        # Compute validation loss and accuracy\n",
    "        val_loss = 0\n",
    "        pred_val = []\n",
    "        true_val = []\n",
    "        for x, y_true in zip(valX, valY):\n",
    "            y, _ = rnn.forward(x)\n",
    "            y = softmax(y)\n",
    "            val_loss += cross_entropy_loss(y, y_true.reshape(-1,1))\n",
    "            pred_val.append(np.argmax(y))\n",
    "            true_val.append(np.argmax(y_true))\n",
    "\n",
    "        # Calculate overall accuracy for each epoch\n",
    "        val_accuracy = accuracy_score(true_val, pred_val)\n",
    "        print('Epoch: %d, Validation Loss: %.4f, Validation Accuracy: %.4f' % \n",
    "              (epoch, val_loss / valX.shape[0], val_accuracy))\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            stop_counter = 0\n",
    "        else:\n",
    "            stop_counter += 1\n",
    "\n",
    "        if stop_counter == early_stop_epochs:\n",
    "            print('Early stopping at epoch: %d' % epoch)\n",
    "            break\n",
    "\n",
    "    # Train set\n",
    "    train_loss = 0\n",
    "    pred_train = []\n",
    "    true_train = []\n",
    "    for x, y_true in zip(trainX, trainY):\n",
    "        y, _ = rnn.forward(x)\n",
    "        y = softmax(y)\n",
    "        train_loss += cross_entropy_loss(y, y_true.reshape(-1,1))\n",
    "        pred_train.append(np.argmax(y))\n",
    "        true_train.append(np.argmax(y_true))\n",
    "    print('Train Loss: %.4f' % (train_loss / trainX.shape[0]))\n",
    "    print(\"Confusion Matrix - Train Set:\\n\", confusion_matrix(true_train, pred_train))\n",
    "\n",
    "\n",
    "trainX, trainY, testX, testY = load_data('data3.h5')\n",
    "trainX, trainY, testX, testY = process_data(trainX, trainY, testX, testY)\n",
    "run_model(trainX, trainY, testX, testY)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Network (RNN)\n",
    "Hidden Dimension = 128\n",
    "With a validation accuracy of 0.2767, the RNN with 128 neurons in the hidden layer stopped early at the 5th epoch. The train loss was found to be high, indicating that the model did not fit the training data well. The confusion matrix also revealed a relatively random distribution of classifications, indicating that the model was unable to differentiate between different activities.\n",
    "\n",
    "Hidden Dimension = 32\n",
    "The RNN's performance was improved by reducing the dimensionality of the hidden layer to 32. The model was terminated at the 15th epoch with a validation accuracy of 0.2800. The confusion matrix, however, revealed that the model had difficulty distinguishing between different types of activities."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Validation Loss: 1.3955, Validation Accuracy: 0.3133\n",
      "Epoch: 1, Validation Loss: 1.4211, Validation Accuracy: 0.3500\n",
      "Epoch: 2, Validation Loss: 1.3328, Validation Accuracy: 0.3600\n",
      "Epoch: 3, Validation Loss: 1.3003, Validation Accuracy: 0.4100\n",
      "Epoch: 4, Validation Loss: 0.8746, Validation Accuracy: 0.6433\n",
      "Epoch: 5, Validation Loss: 1.1585, Validation Accuracy: 0.5433\n",
      "Epoch: 6, Validation Loss: 0.8185, Validation Accuracy: 0.6567\n",
      "Epoch: 7, Validation Loss: 1.7687, Validation Accuracy: 0.2800\n",
      "Epoch: 8, Validation Loss: 1.6035, Validation Accuracy: 0.3100\n",
      "Epoch: 9, Validation Loss: 0.9023, Validation Accuracy: 0.6467\n",
      "Epoch: 10, Validation Loss: 0.9871, Validation Accuracy: 0.5833\n",
      "Epoch: 11, Validation Loss: 0.7710, Validation Accuracy: 0.6767\n",
      "Epoch: 12, Validation Loss: 1.1299, Validation Accuracy: 0.4667\n",
      "Epoch: 13, Validation Loss: 1.1912, Validation Accuracy: 0.5633\n",
      "Epoch: 14, Validation Loss: 1.4348, Validation Accuracy: 0.4000\n",
      "Epoch: 15, Validation Loss: 0.7415, Validation Accuracy: 0.6600\n",
      "Epoch: 16, Validation Loss: 0.6801, Validation Accuracy: 0.7300\n",
      "Epoch: 17, Validation Loss: 0.6377, Validation Accuracy: 0.7267\n",
      "Epoch: 18, Validation Loss: 0.6279, Validation Accuracy: 0.7133\n",
      "Epoch: 19, Validation Loss: 0.5179, Validation Accuracy: 0.7600\n",
      "Epoch: 20, Validation Loss: 0.8946, Validation Accuracy: 0.7067\n",
      "Epoch: 21, Validation Loss: 1.8789, Validation Accuracy: 0.4300\n",
      "Epoch: 22, Validation Loss: 0.9169, Validation Accuracy: 0.5933\n",
      "Epoch: 23, Validation Loss: 1.5054, Validation Accuracy: 0.4067\n",
      "Epoch: 24, Validation Loss: 1.0356, Validation Accuracy: 0.5767\n",
      "Early stopping at epoch: 24\n",
      "Test Loss: 1.0443\n",
      "Confusion Matrix - Test Set:\n",
      " [[ 2 25  1  9 63  0]\n",
      " [ 0 97  0  1  2  0]\n",
      " [ 0  1 97  1  1  0]\n",
      " [ 0  1  3 86 10  0]\n",
      " [ 0 30  8  6 56  0]\n",
      " [ 0 88  0  0 11  1]]\n"
     ]
    }
   ],
   "source": [
    "# 3-b\n",
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "class LSTM:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Xavier initialization\n",
    "        self.Wf = np.random.randn(hidden_dim, hidden_dim + input_dim) / np.sqrt(hidden_dim + input_dim)\n",
    "        self.Wi = np.random.randn(hidden_dim, hidden_dim + input_dim) / np.sqrt(hidden_dim + input_dim)\n",
    "        self.Wc = np.random.randn(hidden_dim, hidden_dim + input_dim) / np.sqrt(hidden_dim + input_dim)\n",
    "        self.Wo = np.random.randn(hidden_dim, hidden_dim + input_dim) / np.sqrt(hidden_dim + input_dim)\n",
    "        self.Wy = np.random.randn(output_dim, hidden_dim) / np.sqrt(hidden_dim)\n",
    "        self.bf = np.zeros((hidden_dim, 1))\n",
    "        self.bi = np.zeros((hidden_dim, 1))\n",
    "        self.bc = np.zeros((hidden_dim, 1))\n",
    "        self.bo = np.zeros((hidden_dim, 1))\n",
    "        self.by = np.zeros((output_dim, 1))\n",
    "        self.last_os = {}\n",
    "        self.last_c_bars = {}\n",
    "        self.last_is = {}\n",
    "        self.last_fs = {}\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        h_prev = np.zeros((self.hidden_dim, 1))\n",
    "        c_prev = np.zeros((self.hidden_dim, 1))\n",
    "        self.last_inputs = inputs\n",
    "        self.last_hs = { 0: h_prev }\n",
    "        self.last_cs = { 0: c_prev }\n",
    "\n",
    "        # Perform forward pass through time step\n",
    "        for i, x in enumerate(inputs):\n",
    "            z = np.row_stack((h_prev, x.reshape(-1, 1)))\n",
    "            f_gate = sigmoid(self.Wf @ z + self.bf)\n",
    "            self.last_fs[i] = f_gate\n",
    "            i_gate = sigmoid(self.Wi @ z + self.bi)\n",
    "            self.last_is[i] = i_gate\n",
    "            c_bar = np.tanh(self.Wc @ z + self.bc)\n",
    "            self.last_c_bars[i] = c_bar\n",
    "            c = f_gate * c_prev + i_gate * c_bar\n",
    "            o_gate = sigmoid(self.Wo @ z + self.bo)\n",
    "            h = o_gate * np.tanh(c)\n",
    "            self.last_hs[i + 1] = h\n",
    "            self.last_cs[i + 1] = c\n",
    "            self.last_os[i + 1] = o_gate  # Store the output gate activation at each time step\n",
    "            h_prev = h\n",
    "            c_prev = c\n",
    "\n",
    "        y = self.Wy @ h + self.by\n",
    "\n",
    "        return y, h\n",
    "\n",
    "    def backward(self, d_y, learn_rate=0.1, momentum=0.85):\n",
    "        # Initialize gradients\n",
    "        d_Wf, d_Wi, d_Wc, d_Wo, d_Wy = np.zeros_like(self.Wf), np.zeros_like(self.Wi), np.zeros_like(self.Wc), np.zeros_like(self.Wo), np.zeros_like(self.Wy)\n",
    "        d_bf, d_bi, d_bc, d_bo, d_by = np.zeros_like(self.bf), np.zeros_like(self.bi), np.zeros_like(self.bc), np.zeros_like(self.bo), np.zeros_like(self.by)\n",
    "\n",
    "        dh_next = np.zeros_like(self.last_hs[0])\n",
    "        dc_next = np.zeros_like(self.last_cs[0])\n",
    "\n",
    "        d_Wy += d_y @ self.last_hs[len(self.last_inputs)].T\n",
    "        d_by += d_y\n",
    "        dh_next += self.Wy.T @ d_y\n",
    "\n",
    "        # Backpropagate through time\n",
    "        for t in reversed(range(1, len(self.last_inputs + 1))):\n",
    "            z = np.row_stack((self.last_hs[t], self.last_inputs[t].reshape(-1, 1)))\n",
    "\n",
    "            dc = dc_next + (dh_next * self.last_os[t] * (1 - np.tanh(self.last_cs[t+1])**2))\n",
    "            do = dh_next * np.tanh(self.last_cs[t+1])\n",
    "            do_input = sigmoid_derivative(self.last_os[t]) * do\n",
    "\n",
    "            di = dc * self.last_c_bars[t]\n",
    "            di_input = sigmoid_derivative(self.last_is[t]) * di\n",
    "            df = dc * self.last_cs[t]\n",
    "            df_input = sigmoid_derivative(self.last_fs[t]) * df\n",
    "            dc_bar = dc * self.last_is[t]\n",
    "            dc_bar_input = (1 - (self.last_c_bars[t])**2) * dc_bar\n",
    "\n",
    "            # Update gradients\n",
    "            dz = (self.Wf.T @ df_input\n",
    "                + self.Wi.T @ di_input\n",
    "                + self.Wc.T @ dc_bar_input\n",
    "                + self.Wo.T @ do_input)\n",
    "            dh_prev = dz[:self.hidden_dim, :]\n",
    "            d_Wf += df_input @ z.T\n",
    "            d_bf += df_input\n",
    "            d_Wi += di_input @ z.T\n",
    "            d_bi += di_input\n",
    "            d_Wc += dc_bar_input @ z.T\n",
    "            d_bc += dc_bar_input\n",
    "            d_Wo += do_input @ z.T\n",
    "            d_bo += do_input\n",
    "\n",
    "            # Prepare for next iteration\n",
    "            dc_next = self.last_fs[t] * dc\n",
    "            dh_next = dh_prev\n",
    "\n",
    "        # Clip to prevent exploding gradients\n",
    "        for d in [d_Wf, d_Wi, d_Wc, d_Wo, d_Wy, d_bf, d_bi, d_bc, d_bo, d_by]:\n",
    "            np.clip(d, -1, 1, out=d)\n",
    "\n",
    "        # Update weights and biases using SGD with momentum\n",
    "        self.Wf -= learn_rate * d_Wf\n",
    "        self.Wi -= learn_rate * d_Wi\n",
    "        self.Wc -= learn_rate * d_Wc\n",
    "        self.Wo -= learn_rate * d_Wo\n",
    "        self.Wy -= learn_rate * d_Wy\n",
    "        self.bf -= learn_rate * d_bf\n",
    "        self.bi -= learn_rate * d_bi\n",
    "        self.bc -= learn_rate * d_bc\n",
    "        self.bo -= learn_rate * d_bo\n",
    "        self.by -= learn_rate * d_by\n",
    "\n",
    "        \n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "# Loss function\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    return -np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "# Load dataset\n",
    "def load_data(filename):\n",
    "    with h5py.File(filename, 'r') as hf:\n",
    "        trainX = np.array(hf.get('trX'))\n",
    "        trainY = np.array(hf.get('trY'))\n",
    "        testX = np.array(hf.get('tstX'))\n",
    "        testY = np.array(hf.get('tstY'))\n",
    "    return trainX, trainY, testX, testY\n",
    "\n",
    "def convert_to_one_hot(y, C):\n",
    "    return np.eye(C)[y.reshape(-1)].T\n",
    "\n",
    "def process_data(trainX, trainY, testX, testY):\n",
    "    assert trainX.shape[0] == trainY.shape[0], \"Mismatch in number of training samples\"\n",
    "    assert testX.shape[0] == testY.shape[0], \"Mismatch in number of test samples\"\n",
    "\n",
    "    # Normalize\n",
    "    trainX /= np.max(trainX)\n",
    "    testX /= np.max(testX)\n",
    "\n",
    "    # Convert labels to integers\n",
    "    trainY = trainY.astype(int)\n",
    "    testY = testY.astype(int)\n",
    "\n",
    "    # One-hot encoding\n",
    "    #trainY = convert_to_one_hot(trainY, 6).T\n",
    "    #testY = convert_to_one_hot(testY, 6).T\n",
    "\n",
    "    return trainX, trainY, testX, testY\n",
    "\n",
    "\n",
    "\n",
    "def run_model(trainX, trainY, testX, testY, hidden_dim=128, epochs=50, mini_batch_size=32, early_stop_epochs=5):\n",
    "    lstm = LSTM(trainX.shape[2], hidden_dim, trainY.shape[1])\n",
    "\n",
    "    # Split training data into training and validation set\n",
    "    trainX, valX, trainY, valY = train_test_split(trainX, trainY, test_size=0.1)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    stop_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Mini-batch gradient descent\n",
    "        mini_batches = [(trainX[k:k+mini_batch_size], trainY[k:k+mini_batch_size]) \n",
    "                        for k in range(0, trainX.shape[0], mini_batch_size)]\n",
    "        \n",
    "        for mini_batch in mini_batches:\n",
    "            X_mini, Y_mini = mini_batch\n",
    "            for x, y_true in zip(X_mini, Y_mini):\n",
    "                y, _ = lstm.forward(x)\n",
    "                y = softmax(y)\n",
    "                error = y - y_true.reshape(-1,1)\n",
    "                lstm.backward(error)\n",
    "\n",
    "        # Compute validation loss and accuracy\n",
    "        val_loss = 0\n",
    "        pred_val = []\n",
    "        true_val = []\n",
    "        for x, y_true in zip(valX, valY):\n",
    "            y, _ = lstm.forward(x)\n",
    "            y = softmax(y)\n",
    "            val_loss += cross_entropy_loss(y, y_true.reshape(-1,1))\n",
    "            pred_val.append(np.argmax(y))\n",
    "            true_val.append(np.argmax(y_true))\n",
    "\n",
    "        # Calculate overall accuracy for each epoch\n",
    "        val_accuracy = accuracy_score(true_val, pred_val)\n",
    "        print('Epoch: %d, Validation Loss: %.4f, Validation Accuracy: %.4f' % \n",
    "              (epoch, val_loss / valX.shape[0], val_accuracy))\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            stop_counter = 0\n",
    "        else:\n",
    "            stop_counter += 1\n",
    "\n",
    "        if stop_counter == early_stop_epochs:\n",
    "            print('Early stopping at epoch: %d' % epoch)\n",
    "            break\n",
    "\n",
    "    # Test model\n",
    "    test_loss = 0\n",
    "    pred_test = []\n",
    "    true_test = []\n",
    "    for x, y_true in zip(testX, testY):\n",
    "        y, _ = lstm.forward(x)\n",
    "        y = softmax(y)\n",
    "        test_loss += cross_entropy_loss(y, y_true.reshape(-1,1))\n",
    "        pred_test.append(np.argmax(y))\n",
    "        true_test.append(np.argmax(y_true))\n",
    "    print('Test Loss: %.4f' % (test_loss / testX.shape[0]))\n",
    "    print(\"Confusion Matrix - Test Set:\\n\", confusion_matrix(true_test, pred_test))\n",
    "\n",
    "\n",
    "trainX, trainY, testX, testY = load_data('data3.h5')\n",
    "trainX, trainY, testX, testY = process_data(trainX, trainY, testX, testY)\n",
    "run_model(trainX, trainY, testX, testY)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) (for hidden_dim = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Validation Loss: 1.3341, Validation Accuracy: 0.4567\n",
      "Epoch: 1, Validation Loss: 1.1370, Validation Accuracy: 0.5267\n",
      "Epoch: 2, Validation Loss: 0.9612, Validation Accuracy: 0.6033\n",
      "Epoch: 3, Validation Loss: 0.8549, Validation Accuracy: 0.6100\n",
      "Epoch: 4, Validation Loss: 0.7589, Validation Accuracy: 0.6433\n",
      "Epoch: 5, Validation Loss: 0.7627, Validation Accuracy: 0.6267\n",
      "Epoch: 6, Validation Loss: 0.9962, Validation Accuracy: 0.6267\n",
      "Epoch: 7, Validation Loss: 0.9156, Validation Accuracy: 0.5967\n",
      "Epoch: 8, Validation Loss: 1.0185, Validation Accuracy: 0.5567\n",
      "Epoch: 9, Validation Loss: 0.7759, Validation Accuracy: 0.6900\n",
      "Early stopping at epoch: 9\n",
      "Test Loss: 0.9464\n",
      "Confusion Matrix - Test Set:\n",
      " [[22  3  4  9 35 27]\n",
      " [ 0 97  0  0  2  1]\n",
      " [ 0  0 99  0  1  0]\n",
      " [ 0  0  9 71 17  3]\n",
      " [11  4  2 15 29 39]\n",
      " [ 5 17  0  0  6 72]]\n"
     ]
    }
   ],
   "source": [
    "# 3-b (for hidden_dim = 32)\n",
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "class LSTM:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Xavier initialization\n",
    "        self.Wf = np.random.randn(hidden_dim, hidden_dim + input_dim) / np.sqrt(hidden_dim + input_dim)\n",
    "        self.Wi = np.random.randn(hidden_dim, hidden_dim + input_dim) / np.sqrt(hidden_dim + input_dim)\n",
    "        self.Wc = np.random.randn(hidden_dim, hidden_dim + input_dim) / np.sqrt(hidden_dim + input_dim)\n",
    "        self.Wo = np.random.randn(hidden_dim, hidden_dim + input_dim) / np.sqrt(hidden_dim + input_dim)\n",
    "        self.Wy = np.random.randn(output_dim, hidden_dim) / np.sqrt(hidden_dim)\n",
    "        self.bf = np.zeros((hidden_dim, 1))\n",
    "        self.bi = np.zeros((hidden_dim, 1))\n",
    "        self.bc = np.zeros((hidden_dim, 1))\n",
    "        self.bo = np.zeros((hidden_dim, 1))\n",
    "        self.by = np.zeros((output_dim, 1))\n",
    "        self.last_os = {}\n",
    "        self.last_c_bars = {}\n",
    "        self.last_is = {}\n",
    "        self.last_fs = {}\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        h_prev = np.zeros((self.hidden_dim, 1))\n",
    "        c_prev = np.zeros((self.hidden_dim, 1))\n",
    "        self.last_inputs = inputs\n",
    "        self.last_hs = { 0: h_prev }\n",
    "        self.last_cs = { 0: c_prev }\n",
    "\n",
    "        # Perform forward pass through time step\n",
    "        for i, x in enumerate(inputs):\n",
    "            z = np.row_stack((h_prev, x.reshape(-1, 1)))\n",
    "            f_gate = sigmoid(self.Wf @ z + self.bf)\n",
    "            self.last_fs[i] = f_gate # Store the forget gate activation at each time step\n",
    "            i_gate = sigmoid(self.Wi @ z + self.bi)\n",
    "            self.last_is[i] = i_gate  # Store the input gate activation at each time step\n",
    "            c_bar = np.tanh(self.Wc @ z + self.bc)\n",
    "            self.last_c_bars[i] = c_bar\n",
    "            c = f_gate * c_prev + i_gate * c_bar\n",
    "            o_gate = sigmoid(self.Wo @ z + self.bo)\n",
    "            h = o_gate * np.tanh(c)\n",
    "            self.last_hs[i + 1] = h\n",
    "            self.last_cs[i + 1] = c\n",
    "            self.last_os[i + 1] = o_gate  # Store the output gate activation at each time step\n",
    "            h_prev = h\n",
    "            c_prev = c\n",
    "\n",
    "        y = self.Wy @ h + self.by\n",
    "\n",
    "        return y, h\n",
    "\n",
    "    def backward(self, d_y, learn_rate=0.1, momentum=0.85):\n",
    "        # Initialize gradients\n",
    "        d_Wf, d_Wi, d_Wc, d_Wo, d_Wy = np.zeros_like(self.Wf), np.zeros_like(self.Wi), np.zeros_like(self.Wc), np.zeros_like(self.Wo), np.zeros_like(self.Wy)\n",
    "        d_bf, d_bi, d_bc, d_bo, d_by = np.zeros_like(self.bf), np.zeros_like(self.bi), np.zeros_like(self.bc), np.zeros_like(self.bo), np.zeros_like(self.by)\n",
    "\n",
    "        dh_next = np.zeros_like(self.last_hs[0])\n",
    "        dc_next = np.zeros_like(self.last_cs[0])\n",
    "\n",
    "        d_Wy += d_y @ self.last_hs[len(self.last_inputs)].T\n",
    "        d_by += d_y\n",
    "        dh_next += self.Wy.T @ d_y\n",
    "\n",
    "        # Backpropagate through time\n",
    "        for t in reversed(range(1, len(self.last_inputs + 1))):\n",
    "            z = np.row_stack((self.last_hs[t], self.last_inputs[t].reshape(-1, 1)))\n",
    "\n",
    "            dc = dc_next + (dh_next * self.last_os[t] * (1 - np.tanh(self.last_cs[t+1])**2))\n",
    "            do = dh_next * np.tanh(self.last_cs[t+1])\n",
    "            do_input = sigmoid_derivative(self.last_os[t]) * do\n",
    "\n",
    "            di = dc * self.last_c_bars[t]\n",
    "            di_input = sigmoid_derivative(self.last_is[t]) * di\n",
    "            df = dc * self.last_cs[t]\n",
    "            df_input = sigmoid_derivative(self.last_fs[t]) * df\n",
    "            dc_bar = dc * self.last_is[t]\n",
    "            dc_bar_input = (1 - (self.last_c_bars[t])**2) * dc_bar\n",
    "\n",
    "            # Update gradients\n",
    "            dz = (self.Wf.T @ df_input\n",
    "                + self.Wi.T @ di_input\n",
    "                + self.Wc.T @ dc_bar_input\n",
    "                + self.Wo.T @ do_input)\n",
    "            dh_prev = dz[:self.hidden_dim, :]\n",
    "            d_Wf += df_input @ z.T\n",
    "            d_bf += df_input\n",
    "            d_Wi += di_input @ z.T\n",
    "            d_bi += di_input\n",
    "            d_Wc += dc_bar_input @ z.T\n",
    "            d_bc += dc_bar_input\n",
    "            d_Wo += do_input @ z.T\n",
    "            d_bo += do_input\n",
    "\n",
    "            # Prepare for next iteration\n",
    "            dc_next = self.last_fs[t] * dc\n",
    "            dh_next = dh_prev\n",
    "\n",
    "        # Clip to prevent exploding gradients\n",
    "        for d in [d_Wf, d_Wi, d_Wc, d_Wo, d_Wy, d_bf, d_bi, d_bc, d_bo, d_by]:\n",
    "            np.clip(d, -1, 1, out=d)\n",
    "\n",
    "        # Update weights and biases using SGD with momentum\n",
    "        self.Wf -= learn_rate * d_Wf\n",
    "        self.Wi -= learn_rate * d_Wi\n",
    "        self.Wc -= learn_rate * d_Wc\n",
    "        self.Wo -= learn_rate * d_Wo\n",
    "        self.Wy -= learn_rate * d_Wy\n",
    "        self.bf -= learn_rate * d_bf\n",
    "        self.bi -= learn_rate * d_bi\n",
    "        self.bc -= learn_rate * d_bc\n",
    "        self.bo -= learn_rate * d_bo\n",
    "        self.by -= learn_rate * d_by\n",
    "\n",
    "        \n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "# Loss function\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    return -np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "# Load dataset\n",
    "def load_data(filename):\n",
    "    with h5py.File(filename, 'r') as hf:\n",
    "        trainX = np.array(hf.get('trX'))\n",
    "        trainY = np.array(hf.get('trY'))\n",
    "        testX = np.array(hf.get('tstX'))\n",
    "        testY = np.array(hf.get('tstY'))\n",
    "    return trainX, trainY, testX, testY\n",
    "\n",
    "def convert_to_one_hot(y, C):\n",
    "    return np.eye(C)[y.reshape(-1)].T\n",
    "\n",
    "def process_data(trainX, trainY, testX, testY):\n",
    "    assert trainX.shape[0] == trainY.shape[0], \"Mismatch in number of training samples\"\n",
    "    assert testX.shape[0] == testY.shape[0], \"Mismatch in number of test samples\"\n",
    "\n",
    "    # Normalize\n",
    "    trainX /= np.max(trainX)\n",
    "    testX /= np.max(testX)\n",
    "\n",
    "    # Convert labels to integers\n",
    "    trainY = trainY.astype(int)\n",
    "    testY = testY.astype(int)\n",
    "\n",
    "    # One-hot encoding\n",
    "    #trainY = convert_to_one_hot(trainY, 6).T\n",
    "    #testY = convert_to_one_hot(testY, 6).T\n",
    "\n",
    "    return trainX, trainY, testX, testY\n",
    "\n",
    "\n",
    "\n",
    "def run_model(trainX, trainY, testX, testY, hidden_dim=32, epochs=50, mini_batch_size=32, early_stop_epochs=5):\n",
    "    lstm = LSTM(trainX.shape[2], hidden_dim, trainY.shape[1])\n",
    "\n",
    "    # Split training data into training and validation set\n",
    "    trainX, valX, trainY, valY = train_test_split(trainX, trainY, test_size=0.1)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    stop_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):      \n",
    "        # Mini-batch gradient descent\n",
    "        mini_batches = [(trainX[k:k+mini_batch_size], trainY[k:k+mini_batch_size]) \n",
    "                        for k in range(0, trainX.shape[0], mini_batch_size)]\n",
    "        \n",
    "        for mini_batch in mini_batches:\n",
    "            X_mini, Y_mini = mini_batch\n",
    "            for x, y_true in zip(X_mini, Y_mini):\n",
    "                y, _ = lstm.forward(x)\n",
    "                y = softmax(y)\n",
    "                error = y - y_true.reshape(-1,1)\n",
    "                lstm.backward(error)\n",
    "\n",
    "        # Compute validation loss and accuracy\n",
    "        val_loss = 0\n",
    "        pred_val = []\n",
    "        true_val = []\n",
    "        for x, y_true in zip(valX, valY):\n",
    "            y, _ = lstm.forward(x)\n",
    "            y = softmax(y)\n",
    "            val_loss += cross_entropy_loss(y, y_true.reshape(-1,1))\n",
    "            pred_val.append(np.argmax(y))\n",
    "            true_val.append(np.argmax(y_true))\n",
    "\n",
    "        # Calculate overall accuracy for each epoch\n",
    "        val_accuracy = accuracy_score(true_val, pred_val)\n",
    "        print('Epoch: %d, Validation Loss: %.4f, Validation Accuracy: %.4f' % \n",
    "              (epoch, val_loss / valX.shape[0], val_accuracy))\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            stop_counter = 0\n",
    "        else:\n",
    "            stop_counter += 1\n",
    "\n",
    "        if stop_counter == early_stop_epochs:\n",
    "            print('Early stopping at epoch: %d' % epoch)\n",
    "            break\n",
    "\n",
    "    # Test model\n",
    "    test_loss = 0\n",
    "    pred_test = []\n",
    "    true_test = []\n",
    "    for x, y_true in zip(testX, testY):\n",
    "        y, _ = lstm.forward(x)\n",
    "        y = softmax(y)\n",
    "        test_loss += cross_entropy_loss(y, y_true.reshape(-1,1))\n",
    "        pred_test.append(np.argmax(y))\n",
    "        true_test.append(np.argmax(y_true))\n",
    "    print('Test Loss: %.4f' % (test_loss / testX.shape[0]))\n",
    "    print(\"Confusion Matrix - Test Set:\\n\", confusion_matrix(true_test, pred_test))\n",
    "\n",
    "\n",
    "trainX, trainY, testX, testY = load_data('data3.h5')\n",
    "trainX, trainY, testX, testY = process_data(trainX, trainY, testX, testY)\n",
    "run_model(trainX, trainY, testX, testY)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long Short-Term Memory (LSTM)\n",
    "Hidden Dimension = 128\n",
    "When compared to the RNN, the LSTM with 128 neurons stopped training at the 24th epoch and achieved a higher validation accuracy of 0.5767. The confusion matrix revealed a more accurate distribution of classifications, implying that LSTM was better able to learn the sequence dependencies within the data.\n",
    "\n",
    "Hidden Dimension = 32\n",
    "The LSTM model with 32 hidden units performed even better, terminating training at the 9th epoch and achieving a higher validation accuracy of 0.6900. The confusion matrix revealed a more accurate classification distribution, implying that reducing the number of hidden neurons improved the model's performance on this problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Validation Loss: 1.5023, Validation Accuracy: 0.3867\n",
      "Epoch: 1, Validation Loss: 1.2880, Validation Accuracy: 0.4767\n",
      "Epoch: 2, Validation Loss: 1.6212, Validation Accuracy: 0.3267\n",
      "Epoch: 3, Validation Loss: 1.3137, Validation Accuracy: 0.4033\n",
      "Epoch: 4, Validation Loss: 1.6081, Validation Accuracy: 0.3133\n",
      "Epoch: 5, Validation Loss: 1.3653, Validation Accuracy: 0.3700\n",
      "Epoch: 6, Validation Loss: 1.9436, Validation Accuracy: 0.1967\n",
      "Early stopping at epoch: 6\n",
      "Test Loss: 2.0229\n",
      "Confusion Matrix - Test Set:\n",
      " [[  0  98   0   0   0   2]\n",
      " [  0  95   0   0   0   5]\n",
      " [  0 100   0   0   0   0]\n",
      " [  0 100   0   0   0   0]\n",
      " [  0  99   0   0   0   1]\n",
      " [  0 100   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "# 3-c\n",
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "class GRU:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Xavier initialization\n",
    "        self.Wz = np.random.randn(hidden_dim, hidden_dim + input_dim) / np.sqrt(hidden_dim + input_dim)\n",
    "        self.Wr = np.random.randn(hidden_dim, hidden_dim + input_dim) / np.sqrt(hidden_dim + input_dim)\n",
    "        self.Wh = np.random.randn(hidden_dim, hidden_dim + input_dim) / np.sqrt(hidden_dim + input_dim)\n",
    "        self.Wy = np.random.randn(output_dim, hidden_dim) / np.sqrt(hidden_dim)\n",
    "        self.bz = np.zeros((hidden_dim, 1))\n",
    "        self.br = np.zeros((hidden_dim, 1))\n",
    "        self.bh = np.zeros((hidden_dim, 1))\n",
    "        self.by = np.zeros((output_dim, 1))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        h_prev = np.zeros((self.hidden_dim, 1))\n",
    "        self.last_inputs = inputs\n",
    "        self.last_hs = { 0: h_prev }\n",
    "        \n",
    "        # Perform forward pass through time step\n",
    "        for i, x in enumerate(inputs):\n",
    "            z = np.row_stack((h_prev, x.reshape(-1, 1)))\n",
    "            z_gate = sigmoid(self.Wz @ z + self.bz)\n",
    "            r_gate = sigmoid(self.Wr @ z + self.br)\n",
    "            h_bar = np.tanh(self.Wh @ np.row_stack((r_gate * h_prev, x.reshape(-1, 1))) + self.bh)\n",
    "            h = ((1 - z_gate) * h_prev) + (z_gate * h_bar)\n",
    "            self.last_hs[i + 1] = h\n",
    "            h_prev = h\n",
    "\n",
    "        y = self.Wy @ h + self.by\n",
    "\n",
    "        return y, h\n",
    "\n",
    "    def backward(self, d_y, learn_rate=0.1, momentum=0.85):\n",
    "        d_Wz, d_Wr, d_Wh, d_Wy = np.zeros_like(self.Wz), np.zeros_like(self.Wr), np.zeros_like(self.Wh), np.zeros_like(self.Wy)\n",
    "        d_bz, d_br, d_bh, d_by = np.zeros_like(self.bz), np.zeros_like(self.br), np.zeros_like(self.bh), np.zeros_like(self.by)\n",
    "\n",
    "        dh_next = np.zeros_like(self.last_hs[0])\n",
    "\n",
    "        d_Wy += d_y @ self.last_hs[len(self.last_inputs)].T\n",
    "        d_by += d_y\n",
    "        dh_next += self.Wy.T @ d_y\n",
    "\n",
    "        for t in reversed(range(1, len(self.last_inputs + 1))):\n",
    "            z = np.row_stack((self.last_hs[t-1], self.last_inputs[t-1].reshape(-1, 1)))\n",
    "\n",
    "            dh = dh_next\n",
    "\n",
    "            z_gate = sigmoid(self.Wz @ z + self.bz)\n",
    "            r_gate = sigmoid(self.Wr @ z + self.br)\n",
    "            h_bar = np.tanh(self.Wh @ np.row_stack((r_gate * self.last_hs[t-1], self.last_inputs[t-1].reshape(-1, 1))) + self.bh)\n",
    "\n",
    "            dh_bar = dh * z_gate\n",
    "            dz_gate = dh * (h_bar - self.last_hs[t-1])\n",
    "            dh_prev = dh * (1 - z_gate)\n",
    "\n",
    "            d_Wh += (1 - np.tanh(h_bar) ** 2) * dh_bar @ np.row_stack((r_gate * self.last_hs[t-1], self.last_inputs[t-1].reshape(-1, 1))).T\n",
    "            d_bh += (1 - np.tanh(h_bar) ** 2) * dh_bar\n",
    "\n",
    "            dr_gate = self.Wh.T @ (1 - np.tanh(h_bar) ** 2) * dh_bar * self.last_hs[t-1]\n",
    "            d_Wr += sigmoid_derivative(r_gate) * dr_gate @ z.T\n",
    "            d_br += sigmoid_derivative(r_gate) * dr_gate\n",
    "\n",
    "            dz_gate = self.Wh.T @ (1 - np.tanh(h_bar) ** 2) * dh_bar * self.last_hs[t-1] + dh * (h_bar - self.last_hs[t-1])\n",
    "            d_Wz += sigmoid_derivative(z_gate) * dz_gate @ z.T\n",
    "            d_bz += sigmoid_derivative(z_gate) * dz_gate\n",
    "\n",
    "            dh_next = self.Wz.T @ sigmoid_derivative(z_gate) * dz_gate + self.Wr.T @ sigmoid_derivative(r_gate) * dr_gate + self.Wh.T @ (1 - np.tanh(h_bar) ** 2) * dh_bar * r_gate + dh * (1 - z_gate)\n",
    "\n",
    "        for d in [d_Wz, d_Wr, d_Wh, d_Wy, d_bz, d_br, d_bh, d_by]:\n",
    "            np.clip(d, -1, 1, out=d)\n",
    "\n",
    "        # Update weights and biases using SGD\n",
    "        self.Wz -= learn_rate * d_Wz\n",
    "        self.Wr -= learn_rate * d_Wr\n",
    "        self.Wh -= learn_rate * d_Wh\n",
    "        self.Wy -= learn_rate * d_Wy\n",
    "        self.bz -= learn_rate * d_bz\n",
    "        self.br -= learn_rate * d_br\n",
    "        self.bh -= learn_rate * d_bh\n",
    "        self.by -= learn_rate * d_by\n",
    "\n",
    "\n",
    "        \n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "# Loss function\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    return -np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "# Load dataset\n",
    "def load_data(filename):\n",
    "    with h5py.File(filename, 'r') as hf:\n",
    "        trainX = np.array(hf.get('trX'))\n",
    "        trainY = np.array(hf.get('trY'))\n",
    "        testX = np.array(hf.get('tstX'))\n",
    "        testY = np.array(hf.get('tstY'))\n",
    "    return trainX, trainY, testX, testY\n",
    "\n",
    "def convert_to_one_hot(y, C):\n",
    "    return np.eye(C)[y.reshape(-1)].T\n",
    "\n",
    "def process_data(trainX, trainY, testX, testY):\n",
    "    assert trainX.shape[0] == trainY.shape[0], \"Mismatch in number of training samples\"\n",
    "    assert testX.shape[0] == testY.shape[0], \"Mismatch in number of test samples\"\n",
    "\n",
    "    # Normalize\n",
    "    trainX /= np.max(trainX)\n",
    "    testX /= np.max(testX)\n",
    "\n",
    "    # Convert labels to integers\n",
    "    trainY = trainY.astype(int)\n",
    "    testY = testY.astype(int)\n",
    "\n",
    "    # One-hot encoding\n",
    "    #trainY = convert_to_one_hot(trainY, 6).T\n",
    "    #testY = convert_to_one_hot(testY, 6).T\n",
    "\n",
    "    return trainX, trainY, testX, testY\n",
    "\n",
    "\n",
    "\n",
    "def run_model(trainX, trainY, testX, testY, hidden_dim=128, epochs=50, mini_batch_size=32, early_stop_epochs=5):\n",
    "    lstm = LSTM(trainX.shape[2], hidden_dim, trainY.shape[1])\n",
    "\n",
    "    # Split training data into training and validation set\n",
    "    trainX, valX, trainY, valY = train_test_split(trainX, trainY, test_size=0.1)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    stop_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Mini-batch gradient descent\n",
    "        mini_batches = [(trainX[k:k+mini_batch_size], trainY[k:k+mini_batch_size]) \n",
    "                        for k in range(0, trainX.shape[0], mini_batch_size)]\n",
    "        \n",
    "        for mini_batch in mini_batches:\n",
    "            X_mini, Y_mini = mini_batch\n",
    "            for x, y_true in zip(X_mini, Y_mini):\n",
    "                y, _ = lstm.forward(x)\n",
    "                y = softmax(y)\n",
    "                error = y - y_true.reshape(-1,1)\n",
    "                lstm.backward(error)\n",
    "\n",
    "        # Compute validation loss and accuracy\n",
    "        val_loss = 0\n",
    "        pred_val = []\n",
    "        true_val = []\n",
    "        for x, y_true in zip(valX, valY):\n",
    "            y, _ = lstm.forward(x)\n",
    "            y = softmax(y)\n",
    "            val_loss += cross_entropy_loss(y, y_true.reshape(-1,1))\n",
    "            pred_val.append(np.argmax(y))\n",
    "            true_val.append(np.argmax(y_true))\n",
    "\n",
    "        # Calculate overall accuracy for each epoch\n",
    "        val_accuracy = accuracy_score(true_val, pred_val)\n",
    "        print('Epoch: %d, Validation Loss: %.4f, Validation Accuracy: %.4f' % \n",
    "              (epoch, val_loss / valX.shape[0], val_accuracy))\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            stop_counter = 0\n",
    "        else:\n",
    "            stop_counter += 1\n",
    "\n",
    "        if stop_counter == early_stop_epochs:\n",
    "            print('Early stopping at epoch: %d' % epoch)\n",
    "            break\n",
    "\n",
    "    # Test model\n",
    "    test_loss = 0\n",
    "    pred_test = []\n",
    "    true_test = []\n",
    "    for x, y_true in zip(testX, testY):\n",
    "        y, _ = lstm.forward(x)\n",
    "        y = softmax(y)\n",
    "        test_loss += cross_entropy_loss(y, y_true.reshape(-1,1))\n",
    "        pred_test.append(np.argmax(y))\n",
    "        true_test.append(np.argmax(y_true))\n",
    "    print('Test Loss: %.4f' % (test_loss / testX.shape[0]))\n",
    "    print(\"Confusion Matrix - Test Set:\\n\", confusion_matrix(true_test, pred_test))\n",
    "\n",
    "\n",
    "trainX, trainY, testX, testY = load_data('data3.h5')\n",
    "trainX, trainY, testX, testY = process_data(trainX, trainY, testX, testY)\n",
    "run_model(trainX, trainY, testX, testY)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) (for hidden_dim = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Validation Loss: 1.3648, Validation Accuracy: 0.4267\n",
      "Epoch: 1, Validation Loss: 1.1264, Validation Accuracy: 0.5033\n",
      "Epoch: 2, Validation Loss: 1.2998, Validation Accuracy: 0.4600\n",
      "Epoch: 3, Validation Loss: 0.9343, Validation Accuracy: 0.6200\n",
      "Epoch: 4, Validation Loss: 0.8801, Validation Accuracy: 0.6233\n",
      "Epoch: 5, Validation Loss: 0.7371, Validation Accuracy: 0.6767\n",
      "Epoch: 6, Validation Loss: 1.0010, Validation Accuracy: 0.5900\n",
      "Epoch: 7, Validation Loss: 0.6247, Validation Accuracy: 0.6900\n",
      "Epoch: 8, Validation Loss: 0.6003, Validation Accuracy: 0.7300\n",
      "Epoch: 9, Validation Loss: 0.7829, Validation Accuracy: 0.6400\n",
      "Epoch: 10, Validation Loss: 1.7034, Validation Accuracy: 0.3533\n",
      "Epoch: 11, Validation Loss: 0.9939, Validation Accuracy: 0.5300\n",
      "Epoch: 12, Validation Loss: 1.5064, Validation Accuracy: 0.3967\n",
      "Epoch: 13, Validation Loss: 0.8988, Validation Accuracy: 0.6033\n",
      "Early stopping at epoch: 13\n",
      "Test Loss: 1.0876\n",
      "Confusion Matrix - Test Set:\n",
      " [[ 82   1   0   5   0  12]\n",
      " [  9  68   1   0   0  22]\n",
      " [  0   0 100   0   0   0]\n",
      " [ 29   0   4  67   0   0]\n",
      " [ 72   1   2  15   0  10]\n",
      " [ 75   9   0   0   0  16]]\n"
     ]
    }
   ],
   "source": [
    "# 3-c (for hidden_dim = 32)\n",
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "class GRU:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Xavier initialization\n",
    "        self.Wz = np.random.randn(hidden_dim, hidden_dim + input_dim) / np.sqrt(hidden_dim + input_dim)\n",
    "        self.Wr = np.random.randn(hidden_dim, hidden_dim + input_dim) / np.sqrt(hidden_dim + input_dim)\n",
    "        self.Wh = np.random.randn(hidden_dim, hidden_dim + input_dim) / np.sqrt(hidden_dim + input_dim)\n",
    "        self.Wy = np.random.randn(output_dim, hidden_dim) / np.sqrt(hidden_dim)\n",
    "        self.bz = np.zeros((hidden_dim, 1))\n",
    "        self.br = np.zeros((hidden_dim, 1))\n",
    "        self.bh = np.zeros((hidden_dim, 1))\n",
    "        self.by = np.zeros((output_dim, 1))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        h_prev = np.zeros((self.hidden_dim, 1))\n",
    "        self.last_inputs = inputs\n",
    "        self.last_hs = { 0: h_prev }\n",
    "        \n",
    "        # Perform forward pass through time step\n",
    "        for i, x in enumerate(inputs):\n",
    "            z = np.row_stack((h_prev, x.reshape(-1, 1)))\n",
    "            z_gate = sigmoid(self.Wz @ z + self.bz)\n",
    "            r_gate = sigmoid(self.Wr @ z + self.br)\n",
    "            h_bar = np.tanh(self.Wh @ np.row_stack((r_gate * h_prev, x.reshape(-1, 1))) + self.bh)\n",
    "            h = ((1 - z_gate) * h_prev) + (z_gate * h_bar)\n",
    "            self.last_hs[i + 1] = h\n",
    "            h_prev = h\n",
    "\n",
    "        y = self.Wy @ h + self.by\n",
    "\n",
    "        return y, h\n",
    "\n",
    "    def backward(self, d_y, learn_rate=0.1, momentum=0.85):\n",
    "        d_Wz, d_Wr, d_Wh, d_Wy = np.zeros_like(self.Wz), np.zeros_like(self.Wr), np.zeros_like(self.Wh), np.zeros_like(self.Wy)\n",
    "        d_bz, d_br, d_bh, d_by = np.zeros_like(self.bz), np.zeros_like(self.br), np.zeros_like(self.bh), np.zeros_like(self.by)\n",
    "\n",
    "        dh_next = np.zeros_like(self.last_hs[0])\n",
    "\n",
    "        d_Wy += d_y @ self.last_hs[len(self.last_inputs)].T\n",
    "        d_by += d_y\n",
    "        dh_next += self.Wy.T @ d_y\n",
    "\n",
    "        for t in reversed(range(1, len(self.last_inputs + 1))):\n",
    "            z = np.row_stack((self.last_hs[t-1], self.last_inputs[t-1].reshape(-1, 1)))\n",
    "\n",
    "            dh = dh_next\n",
    "\n",
    "            z_gate = sigmoid(self.Wz @ z + self.bz)\n",
    "            r_gate = sigmoid(self.Wr @ z + self.br)\n",
    "            h_bar = np.tanh(self.Wh @ np.row_stack((r_gate * self.last_hs[t-1], self.last_inputs[t-1].reshape(-1, 1))) + self.bh)\n",
    "\n",
    "            dh_bar = dh * z_gate\n",
    "            dz_gate = dh * (h_bar - self.last_hs[t-1])\n",
    "            dh_prev = dh * (1 - z_gate)\n",
    "\n",
    "            d_Wh += (1 - np.tanh(h_bar) ** 2) * dh_bar @ np.row_stack((r_gate * self.last_hs[t-1], self.last_inputs[t-1].reshape(-1, 1))).T\n",
    "            d_bh += (1 - np.tanh(h_bar) ** 2) * dh_bar\n",
    "\n",
    "            dr_gate = self.Wh.T @ (1 - np.tanh(h_bar) ** 2) * dh_bar * self.last_hs[t-1]\n",
    "            d_Wr += sigmoid_derivative(r_gate) * dr_gate @ z.T\n",
    "            d_br += sigmoid_derivative(r_gate) * dr_gate\n",
    "\n",
    "            dz_gate = self.Wh.T @ (1 - np.tanh(h_bar) ** 2) * dh_bar * self.last_hs[t-1] + dh * (h_bar - self.last_hs[t-1])\n",
    "            d_Wz += sigmoid_derivative(z_gate) * dz_gate @ z.T\n",
    "            d_bz += sigmoid_derivative(z_gate) * dz_gate\n",
    "\n",
    "            dh_next = self.Wz.T @ sigmoid_derivative(z_gate) * dz_gate + self.Wr.T @ sigmoid_derivative(r_gate) * dr_gate + self.Wh.T @ (1 - np.tanh(h_bar) ** 2) * dh_bar * r_gate + dh * (1 - z_gate)\n",
    "\n",
    "        for d in [d_Wz, d_Wr, d_Wh, d_Wy, d_bz, d_br, d_bh, d_by]:\n",
    "            np.clip(d, -1, 1, out=d)\n",
    "\n",
    "        # Update weights and biases using SGD\n",
    "        self.Wz -= learn_rate * d_Wz\n",
    "        self.Wr -= learn_rate * d_Wr\n",
    "        self.Wh -= learn_rate * d_Wh\n",
    "        self.Wy -= learn_rate * d_Wy\n",
    "        self.bz -= learn_rate * d_bz\n",
    "        self.br -= learn_rate * d_br\n",
    "        self.bh -= learn_rate * d_bh\n",
    "        self.by -= learn_rate * d_by\n",
    "\n",
    "\n",
    "        \n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "# Loss function\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    return -np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "# Load dataset\n",
    "def load_data(filename):\n",
    "    with h5py.File(filename, 'r') as hf:\n",
    "        trainX = np.array(hf.get('trX'))\n",
    "        trainY = np.array(hf.get('trY'))\n",
    "        testX = np.array(hf.get('tstX'))\n",
    "        testY = np.array(hf.get('tstY'))\n",
    "    return trainX, trainY, testX, testY\n",
    "\n",
    "def convert_to_one_hot(y, C):\n",
    "    return np.eye(C)[y.reshape(-1)].T\n",
    "\n",
    "def process_data(trainX, trainY, testX, testY):\n",
    "    assert trainX.shape[0] == trainY.shape[0], \"Mismatch in number of training samples\"\n",
    "    assert testX.shape[0] == testY.shape[0], \"Mismatch in number of test samples\"\n",
    "\n",
    "    # Normalize\n",
    "    trainX /= np.max(trainX)\n",
    "    testX /= np.max(testX)\n",
    "\n",
    "    # Convert labels to integers\n",
    "    trainY = trainY.astype(int)\n",
    "    testY = testY.astype(int)\n",
    "\n",
    "    # One-hot encoding\n",
    "    #trainY = convert_to_one_hot(trainY, 6).T\n",
    "    #testY = convert_to_one_hot(testY, 6).T\n",
    "\n",
    "    return trainX, trainY, testX, testY\n",
    "\n",
    "\n",
    "\n",
    "def run_model(trainX, trainY, testX, testY, hidden_dim=32, epochs=50, mini_batch_size=32, early_stop_epochs=5):\n",
    "    lstm = LSTM(trainX.shape[2], hidden_dim, trainY.shape[1])\n",
    "\n",
    "    # Split training data into training and validation set\n",
    "    trainX, valX, trainY, valY = train_test_split(trainX, trainY, test_size=0.1)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    stop_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Mini-batch gradient descent\n",
    "        mini_batches = [(trainX[k:k+mini_batch_size], trainY[k:k+mini_batch_size]) \n",
    "                        for k in range(0, trainX.shape[0], mini_batch_size)]\n",
    "        \n",
    "        for mini_batch in mini_batches:\n",
    "            X_mini, Y_mini = mini_batch\n",
    "            for x, y_true in zip(X_mini, Y_mini):\n",
    "                y, _ = lstm.forward(x)\n",
    "                y = softmax(y)\n",
    "                error = y - y_true.reshape(-1,1)\n",
    "                lstm.backward(error)\n",
    "\n",
    "        # Compute validation loss and accuracy\n",
    "        val_loss = 0\n",
    "        pred_val = []\n",
    "        true_val = []\n",
    "        for x, y_true in zip(valX, valY):\n",
    "            y, _ = lstm.forward(x)\n",
    "            y = softmax(y)\n",
    "            val_loss += cross_entropy_loss(y, y_true.reshape(-1,1))\n",
    "            pred_val.append(np.argmax(y))\n",
    "            true_val.append(np.argmax(y_true))\n",
    "\n",
    "        # Calculate overall accuracy for each epoch\n",
    "        val_accuracy = accuracy_score(true_val, pred_val)\n",
    "        print('Epoch: %d, Validation Loss: %.4f, Validation Accuracy: %.4f' % \n",
    "              (epoch, val_loss / valX.shape[0], val_accuracy))\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            stop_counter = 0\n",
    "        else:\n",
    "            stop_counter += 1\n",
    "\n",
    "        if stop_counter == early_stop_epochs:\n",
    "            print('Early stopping at epoch: %d' % epoch)\n",
    "            break\n",
    "\n",
    "    # Test model\n",
    "    test_loss = 0\n",
    "    pred_test = []\n",
    "    true_test = []\n",
    "    for x, y_true in zip(testX, testY):\n",
    "        y, _ = lstm.forward(x)\n",
    "        y = softmax(y)\n",
    "        test_loss += cross_entropy_loss(y, y_true.reshape(-1,1))\n",
    "        pred_test.append(np.argmax(y))\n",
    "        true_test.append(np.argmax(y_true))\n",
    "    print('Test Loss: %.4f' % (test_loss / testX.shape[0]))\n",
    "    print(\"Confusion Matrix - Test Set:\\n\", confusion_matrix(true_test, pred_test))\n",
    "\n",
    "\n",
    "trainX, trainY, testX, testY = load_data('data3.h5')\n",
    "trainX, trainY, testX, testY = process_data(trainX, trainY, testX, testY)\n",
    "run_model(trainX, trainY, testX, testY)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gated Recurrent Unit (GRU)\n",
    "Hidden Dimension = 128\n",
    "The GRU model with 128 hidden units performed poorly, with training stopping at the sixth epoch and a validation accuracy of 0.1967. According to the confusion matrix, the model struggled with the classification task, most likely due to overfitting or an inability to learn the dependencies within the sequence data.\n",
    "\n",
    "Hidden Dimension = 32\n",
    "The GRU model with 32 hidden neurons outperformed the others, with training stopping at the 13th epoch and a validation accuracy of 0.6033. The confusion matrix revealed a better classification distribution, implying that reducing the number of hidden neurons improved the model's performance for this problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of all configurations, the LSTM and GRU models with 32 hidden units performed the best. Both models had higher validation accuracy and better confusion matrices, indicating that they were better at learning the dependencies within the sequence data and distinguishing between different activities.\n",
    "\n",
    "Although the LSTM and GRU models with 128 hidden units did not perform as well as the RNN models, they outperformed them. This suggests that for this type of time-series classification problem, the ability of LSTM and GRU to control the flow of information through time and retain important historical information from the sequence data provides significant advantages over a simple RNN model.\n",
    "\n",
    "The best performance was achieved by the LSTM model with 32 hidden neurons, which had the highest validation accuracy and a confusion matrix indicating a good classification distribution. The LSTM's mechanism for controlling and retaining information over time may make it particularly well suited to this problem.\n",
    "\n",
    "Each model's confusion matrix provides information about the model's specific strengths and weaknesses. The RNN models struggled to distinguish between different activities, resulting in poor performance. The LSTM and GRU models, on the other hand, performed better in distinguishing between activities, as evidenced by a greater concentration of values along the diagonal of the confusion matrices.\n",
    "\n",
    "In conclusion, due to their ability to handle time-series data, LSTM and GRU appear to be better choices than a simple RNN for this human activity classification task. The choice between LSTM and GRU may be influenced by the number of hidden dimensions and computational resources available, with LSTM requiring more computational resources but potentially providing better performance with larger hidden dimensions.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
