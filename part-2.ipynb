{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, validation loss: 4.552673251145384\n",
      "Epoch 2/50, validation loss: 4.48011218232445\n",
      "Epoch 3/50, validation loss: 4.451650452155833\n",
      "Epoch 4/50, validation loss: 4.432681853287966\n",
      "Epoch 5/50, validation loss: 4.421032152952502\n",
      "Epoch 6/50, validation loss: 4.413389655255122\n",
      "Epoch 7/50, validation loss: 4.408029502175557\n",
      "Epoch 8/50, validation loss: 4.404063835037568\n",
      "Epoch 9/50, validation loss: 4.401007091597594\n",
      "Epoch 10/50, validation loss: 4.398575475276475\n",
      "Epoch 11/50, validation loss: 4.396593017774308\n",
      "Epoch 12/50, validation loss: 4.394944945901273\n",
      "Epoch 13/50, validation loss: 4.393553118072547\n",
      "Epoch 14/50, validation loss: 4.392362369927801\n",
      "Epoch 15/50, validation loss: 4.391332548971821\n",
      "Epoch 16/50, validation loss: 4.390433663492605\n",
      "Epoch 17/50, validation loss: 4.389642812945535\n",
      "Epoch 18/50, validation loss: 4.388942178746412\n",
      "Epoch 19/50, validation loss: 4.388317669289839\n",
      "Epoch 20/50, validation loss: 4.387757981777355\n",
      "Epoch 21/50, validation loss: 4.387253937344302\n",
      "Epoch 22/50, validation loss: 4.3867980000546565\n",
      "Epoch 23/50, validation loss: 4.3863839224769885\n",
      "Epoch 24/50, validation loss: 4.386006480220417\n",
      "Epoch 25/50, validation loss: 4.38566127016198\n",
      "Epoch 26/50, validation loss: 4.385344555043722\n",
      "Epoch 27/50, validation loss: 4.385053142344314\n",
      "Epoch 28/50, validation loss: 4.3847842888362\n",
      "Epoch 29/50, validation loss: 4.384535624634325\n",
      "Epoch 30/50, validation loss: 4.384305092207221\n",
      "Epoch 31/50, validation loss: 4.384090896995183\n"
     ]
    }
   ],
   "source": [
    "# Question 2-a\n",
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def load_data(filename):\n",
    "    with h5py.File(filename, 'r') as hf:\n",
    "        trainx = np.array(hf['trainx'])\n",
    "        traind = np.array(hf['traind'])\n",
    "        valx = np.array(hf['valx'])\n",
    "        vald = np.array(hf['vald'])\n",
    "        testx = np.array(hf['testx'])\n",
    "        testd = np.array(hf['testd'])\n",
    "    return trainx, traind, valx, vald, testx, testd\n",
    "\n",
    "def one_hot_encode(data, vocab_size):\n",
    "    encoder = OneHotEncoder(sparse=False, categories=[range(vocab_size)])\n",
    "    return encoder.fit_transform(data.reshape(-1, 1))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy(y_pred, y_true):\n",
    "    return -np.sum(y_true * np.log(y_pred + 1e-8)) / y_true.shape[0]\n",
    "\n",
    "def init_params(input_size, hidden_size, output_size, vocab_size, embedding_dim):\n",
    "    params = {}\n",
    "    params['E'] = np.random.normal(0, 0.01, (vocab_size + 1, embedding_dim))\n",
    "    params['W1'] = np.random.normal(0, 0.01, (embedding_dim, hidden_size))\n",
    "    params['b1'] = np.zeros((1, hidden_size))\n",
    "    params['W2'] = np.random.normal(0, 0.01, (hidden_size, output_size))\n",
    "    params['b2'] = np.zeros((1, output_size))\n",
    "    return params\n",
    "\n",
    "def forward_backward(params, x_batch, y_batch, vocab_size, momentum=0.85, learning_rate=0.15):\n",
    "    # Forward pass\n",
    "    E, W1, b1, W2, b2 = params['E'], params['W1'], params['b1'], params['W2'], params['b2']\n",
    "    x_embedded = np.sum(E[x_batch], axis=1)\n",
    "    h = sigmoid(x_embedded.dot(W1) + b1)\n",
    "    y_pred = softmax(h.dot(W2) + b2)\n",
    "    grads_prev = {key: np.zeros_like(val) for key, val in params.items()}\n",
    "\n",
    "    # Backward pass\n",
    "    grads = {}\n",
    "    d_y = (y_pred - y_batch) / y_batch.shape[0]\n",
    "    grads['W2'] = h.T.dot(d_y) + momentum * W2\n",
    "    grads['b2'] = np.sum(d_y, axis=0, keepdims=True)\n",
    "\n",
    "    d_h = d_y.dot(W2.T) * (h * (1 - h))\n",
    "    grads['W1'] = x_embedded.T.dot(d_h) + momentum * W1\n",
    "    grads['b1'] = np.sum(d_h, axis=0, keepdims=True)\n",
    "\n",
    "    d_E = np.zeros_like(E)\n",
    "    for i, x in enumerate(x_batch):\n",
    "        d_embedded = d_h[i].dot(W1.T)\n",
    "        d_E[x] += d_embedded\n",
    "\n",
    "    grads['E'] = d_E + momentum * E\n",
    "\n",
    "    # Update params\n",
    "    params['W2'] -= learning_rate * grads['W2']\n",
    "    params['b2'] -= learning_rate * grads['b2']\n",
    "    params['W1'] -= learning_rate * grads['W1']\n",
    "    params['b1'] -= learning_rate * grads['b1']\n",
    "    params['E'] -= learning_rate * grads['E']\n",
    "\n",
    "    for key in params:\n",
    "        params[key] -= learning_rate * (grads[key] + momentum * grads_prev[key])\n",
    "        grads_prev[key] = grads[key]\n",
    "        \n",
    "    return params, y_pred\n",
    "\n",
    "def train_network(trainx, traind, valx, vald, testx, testd, vocab_size, embedding_dim, hidden_size, epochs=50, batch_size=200):\n",
    "    trainy = one_hot_encode(traind, vocab_size + 1)\n",
    "    valy = one_hot_encode(vald, vocab_size + 1)\n",
    "    testy = one_hot_encode(testd, vocab_size + 1)\n",
    "    params = init_params(embedding_dim, hidden_size, vocab_size + 1, vocab_size, embedding_dim)\n",
    "    prev_val_loss = float('inf')\n",
    "    tolerance = 1e-4\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for batch_start in range(0, trainx.shape[0], batch_size):\n",
    "            x_batch = trainx[batch_start:batch_start + batch_size]\n",
    "            y_batch = trainy[batch_start:batch_start + batch_size]\n",
    "            params, _ = forward_backward(params, x_batch, y_batch, vocab_size)\n",
    "\n",
    "        _, val_pred = forward_backward(params, valx, valy, vocab_size)\n",
    "        val_loss = cross_entropy(val_pred, valy)\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, validation loss: {val_loss}')\n",
    "        if prev_val_loss - val_loss < tolerance:\n",
    "            print('Training stopped due to early stopping')\n",
    "            break\n",
    "        prev_val_loss = val_loss\n",
    "    _, test_pred = forward_backward(params, testx, testy, vocab_size)\n",
    "    test_loss = cross_entropy(test_pred, testy)\n",
    "    print(f'Test loss: {test_loss}')\n",
    "\n",
    "    return params\n",
    "\n",
    "def predict(params, trigrams, vocab_size, top_k=10):\n",
    "    E, W1, b1, W2, b2 = params['E'], params['W1'], params['b1'], params['W2'], params['b2']\n",
    "    x_embedded = np.sum(E[trigrams], axis=1)\n",
    "    h = sigmoid(x_embedded.dot(W1) + b1)\n",
    "    y_pred = softmax(h.dot(W2) + b2)\n",
    "\n",
    "    top_k_indices = np.argsort(y_pred, axis=1)[:, -top_k:]\n",
    "    return top_k_indices, y_pred\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    trainx, traind, valx, vald, testx, testd = load_data('data2.h5')\n",
    "    vocab_size = 250\n",
    "    embedding_dim = 32\n",
    "    hidden_size = 256\n",
    "    params = train_network(trainx, traind, valx, vald, testx, testd, vocab_size, embedding_dim, hidden_size)\n",
    "\n",
    "    # Sample trigrams\n",
    "    sample_trigrams = testx[:5]\n",
    "    top_k_indices, _ = predict(params, sample_trigrams, vocab_size)\n",
    "    for i, trigram in enumerate(sample_trigrams):\n",
    "        print(f'Trigram: {trigram}, top 10 predictions: {top_k_indices[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, validation loss: 4.457927513131857\n",
      "Epoch 2/50, validation loss: 4.416193458300157\n",
      "Epoch 3/50, validation loss: 4.402336021228595\n",
      "Epoch 4/50, validation loss: 4.395881140557717\n",
      "Epoch 5/50, validation loss: 4.392236400205425\n",
      "Epoch 6/50, validation loss: 4.389922685807068\n",
      "Epoch 7/50, validation loss: 4.3883359894360225\n",
      "Epoch 8/50, validation loss: 4.3871866473629035\n",
      "Epoch 9/50, validation loss: 4.38631923063864\n",
      "Epoch 10/50, validation loss: 4.3856432124526465\n",
      "Epoch 11/50, validation loss: 4.385102485679296\n",
      "Epoch 12/50, validation loss: 4.384660544001437\n",
      "Epoch 13/50, validation loss: 4.384292688905736\n",
      "Epoch 14/50, validation loss: 4.383981665356584\n",
      "Epoch 15/50, validation loss: 4.383715089142423\n",
      "Epoch 16/50, validation loss: 4.383483864763468\n",
      "Epoch 17/50, validation loss: 4.38328117729137\n",
      "Epoch 18/50, validation loss: 4.383101830244033\n",
      "Epoch 19/50, validation loss: 4.382941799208101\n",
      "Epoch 20/50, validation loss: 4.382797923953909\n",
      "Epoch 21/50, validation loss: 4.382667691737334\n",
      "Epoch 22/50, validation loss: 4.38254908200059\n",
      "Epoch 23/50, validation loss: 4.382440453248801\n",
      "Epoch 24/50, validation loss: 4.382340459422105\n",
      "Epoch 25/50, validation loss: 4.382247987234096\n",
      "Epoch 26/50, validation loss: 4.382162108636289\n",
      "Epoch 27/50, validation loss: 4.3820820443455855\n",
      "Epoch 28/50, validation loss: 4.382007135564873\n",
      "Epoch 29/50, validation loss: 4.381936821842861\n",
      "Epoch 30/50, validation loss: 4.38187062358416\n",
      "Epoch 31/50, validation loss: 4.381808128117097\n",
      "Epoch 32/50, validation loss: 4.381748978509538\n",
      "Epoch 33/50, validation loss: 4.381692864525659\n",
      "Epoch 34/50, validation loss: 4.381639515264634\n",
      "Epoch 35/50, validation loss: 4.381588693130709\n",
      "Epoch 36/50, validation loss: 4.381540188864752\n",
      "Epoch 37/50, validation loss: 4.381493817427842\n",
      "Epoch 38/50, validation loss: 4.3814494145732406\n",
      "Epoch 39/50, validation loss: 4.381406833977427\n",
      "Epoch 40/50, validation loss: 4.381365944828126\n",
      "Epoch 41/50, validation loss: 4.381326629787713\n",
      "Epoch 42/50, validation loss: 4.381288783266301\n",
      "Epoch 43/50, validation loss: 4.381252309951742\n",
      "Epoch 44/50, validation loss: 4.381217123553651\n",
      "Epoch 45/50, validation loss: 4.381183145726292\n",
      "Epoch 46/50, validation loss: 4.381150305141865\n",
      "Epoch 47/50, validation loss: 4.381118536690333\n",
      "Epoch 48/50, validation loss: 4.381087780786519\n",
      "Epoch 49/50, validation loss: 4.381057982768105\n",
      "Epoch 50/50, validation loss: 4.381029092371117\n",
      "Test loss: 4.387123471865306\n",
      "Trigram: [183  90 187], top 10 predictions: [158  75 170 249  32 193  89  23  26 144]\n",
      "Trigram: [ 43  68 243], top 10 predictions: [158  75 170 249  32 193  89  23  26 144]\n",
      "Trigram: [ 68 243  23], top 10 predictions: [158  75 170 249  32 193  89  23  26 144]\n",
      "Trigram: [243  23  55], top 10 predictions: [158  75 170 249  32 193  89  23  26 144]\n",
      "Trigram: [23 55 75], top 10 predictions: [158  75 170 249  32 193  89  23  26 144]\n",
      "Epoch 1/50, validation loss: 4.4561606183271385\n",
      "Epoch 2/50, validation loss: 4.413366562480229\n",
      "Epoch 3/50, validation loss: 4.399715082679116\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    trainx, traind, valx, vald, testx, testd = load_data('data2.h5')\n",
    "    vocab_size = 250\n",
    "    embedding_dim = 16\n",
    "    hidden_size = 128\n",
    "    params = train_network(trainx, traind, valx, vald, testx, testd, vocab_size, embedding_dim, hidden_size)\n",
    "    \n",
    "    # Sample trigrams\n",
    "    sample_trigrams = testx[:5]\n",
    "    top_k_indices, _ = predict(params, sample_trigrams, vocab_size)\n",
    "    for i, trigram in enumerate(sample_trigrams):\n",
    "        print(f'Trigram: {trigram}, top 10 predictions: {top_k_indices[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, validation loss: 4.416578425191115\n",
      "Epoch 2/50, validation loss: 4.3964533995665525\n",
      "Epoch 3/50, validation loss: 4.390498078495738\n",
      "Epoch 4/50, validation loss: 4.387706701942462\n",
      "Epoch 5/50, validation loss: 4.386095014614435\n",
      "Epoch 6/50, validation loss: 4.385043736039718\n",
      "Epoch 7/50, validation loss: 4.384300036142249\n",
      "Epoch 8/50, validation loss: 4.383742577842868\n",
      "Epoch 9/50, validation loss: 4.383306351363335\n",
      "Epoch 10/50, validation loss: 4.382953574413042\n",
      "Epoch 11/50, validation loss: 4.382660854170203\n",
      "Epoch 12/50, validation loss: 4.382412950348726\n",
      "Epoch 13/50, validation loss: 4.382199508569629\n",
      "Epoch 14/50, validation loss: 4.382013241933255\n",
      "Epoch 15/50, validation loss: 4.381848866144871\n",
      "Epoch 16/50, validation loss: 4.381702448626958\n",
      "Epoch 17/50, validation loss: 4.38157099582696\n",
      "Epoch 18/50, validation loss: 4.381452183143201\n",
      "Epoch 19/50, validation loss: 4.381344173246579\n",
      "Epoch 20/50, validation loss: 4.381245490869842\n",
      "Training stopped due to early stopping\n",
      "Test loss: 4.386875865337153\n",
      "Trigram: [183  90 187], top 10 predictions: [158 170  75 249  32 193  89  26  23 144]\n",
      "Trigram: [ 43  68 243], top 10 predictions: [158 170  75 249  32 193  89  26  23 144]\n",
      "Trigram: [ 68 243  23], top 10 predictions: [158 170  75 249  32 193  89  26  23 144]\n",
      "Trigram: [243  23  55], top 10 predictions: [158 170  75 249  32 193  89  26  23 144]\n",
      "Trigram: [23 55 75], top 10 predictions: [158 170  75 249  32 193  89  26  23 144]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    trainx, traind, valx, vald, testx, testd = load_data('data2.h5')\n",
    "    vocab_size = 250\n",
    "    embedding_dim = 8\n",
    "    hidden_size = 64\n",
    "    params = train_network(trainx, traind, valx, vald, testx, testd, vocab_size, embedding_dim, hidden_size)\n",
    "\n",
    "    # Sample trigrams\n",
    "    sample_trigrams = testx[:5]\n",
    "    top_k_indices, _ = predict(params, sample_trigrams, vocab_size)\n",
    "    for i, trigram in enumerate(sample_trigrams):\n",
    "        print(f'Trigram: {trigram}, top 10 predictions: {top_k_indices[i]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models trained with (D, P) = (32,256), (16,128) and (8,64) value configurations all show promise as they reduce validation loss over epochs, indicating learning from training data.\n",
    "\n",
    "(32,256): The model consistently improves its performance over 31 epochs, demonstrating a strong ability to learn from data.\n",
    "\n",
    "\n",
    "(16,128): This model achieves comparable validation loss levels in 31 epochs, indicating potentially faster learning and efficient training time.\n",
    "\n",
    "(8,64): Despite being the smallest model, it performs well, achieving a similar validation loss level in only 20 epochs due to early stopping.\n",
    "\n",
    "These findings are encouraging. To improve further, we can test larger models and fine-tune learning rates too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram: only them said\n",
      "Top 10 predictions for the fourth word:\n",
      "  Index 250 not found in vocabulary\n",
      "  its\n",
      "  after\n",
      "  we\n",
      "  be\n",
      "  ?\n",
      "  public\n",
      "  three\n",
      "  ;\n",
      "  new\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Question 2-b\n",
    "import numpy as np\n",
    "\n",
    "# sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# softmax function\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def predict(params, x_batch, vocab_size):\n",
    "    # Forward pass\n",
    "    E, W1, b1, W2, b2 = params['E'], params['W1'], params['b1'], params['W2'], params['b2']\n",
    "    x_embedded = np.sum(E[x_batch], axis=1)\n",
    "    h = sigmoid(x_embedded.dot(W1) + b1)\n",
    "    y_pred = softmax(h.dot(W2) + b2)\n",
    "    return y_pred\n",
    "\n",
    "def top_k_predictions(pred_probs, k=10):\n",
    "    return np.argsort(pred_probs, axis=1)[:, -k:]\n",
    "\n",
    "# load the data\n",
    "with h5py.File('data2.h5', 'r') as hf:\n",
    "    words = hf['words'][()]\n",
    "    testx = hf['testx'][()]\n",
    "\n",
    "# decode bytes to string\n",
    "words = [word.decode('utf-8') for word in words]\n",
    "word2index = {word: index for index, word in enumerate(words)}\n",
    "index2word = {index: word for index, word in enumerate(words)}\n",
    "\n",
    "# pick some sample trigrams from the test data\n",
    "sample_indices = np.random.choice(len(testx), size=1, replace=False)\n",
    "sample_trigrams = testx[sample_indices]\n",
    "\n",
    "# generate predictions for the fourth word\n",
    "pred_probs = predict(params, sample_trigrams, len(words))\n",
    "top_10_preds = top_k_predictions(pred_probs, k=10)\n",
    "\n",
    "# print the top 10 candidates for each sample trigram\n",
    "for i, trigram in enumerate(sample_trigrams):\n",
    "    print(f'Trigram: {\" \".join(index2word[idx] for idx in trigram)}')\n",
    "    print('Top 10 predictions for the fourth word:')\n",
    "    for pred in reversed(top_10_preds[i]):\n",
    "        if pred in index2word:\n",
    "            print(f'  {index2word[pred]}')\n",
    "        else:\n",
    "            print(f'  Index {pred} not found in vocabulary')\n",
    "    print()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model'sÂ predictions for \"only them said\" seem reasonable. Depending on the situation, phrases like \"only them said its,\" \"only them said after,\" or \"only them said we\" might make sense. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
